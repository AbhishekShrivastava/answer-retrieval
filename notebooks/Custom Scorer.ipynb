{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start creating custom scorers, make sure you have created and configured a Solr cluster and trained a ranker by either following the steps on the notebook \"Answer-Retrieval.ipynb\" or by using some other tooling. \n",
    "\n",
    "If you used the python notebook mentioned above, you are ready to start. Otherwise, please make sure you enter the needed information into the file credentials.json present in the same directory of this notebook. This will allow the exercise performed here to read and write needed credentials and constant values for all the steps ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Custom Scorer\n",
    "\n",
    "Custom Scorers should be defined and configured within custom-scorer folder.\n",
    "\n",
    "In this step, you will create a custom feature to improve the ranking phase. In general, there can be multiple custom features that can be engineered depending upon the use case.\n",
    "\n",
    "The custom scorer on this tutorial adds a feature related to the number of up votes an answer received on Stack Exchange. This feature is related to the content of a document. You can also create scorers that work on the content of the query or on both (query and document).\n",
    "\n",
    "The class that implements your scorer can be added to the corresponding package (one of document, query, query_document) in the rr_custom_scorers project. This is how the class for the Up votes scorer looks like:\n",
    "\n",
    "--------------------------------------------------------------------------\n",
    "from document_scorer import DocumentScorer\n",
    "\n",
    "class UpVoteScorer(DocumentScorer):\n",
    "\n",
    "    def __init__(self, name='DocumentScorer', short_name='ds', description='Description of the scorer',\n",
    "                 include_stop=False):\n",
    "        \"\"\" Base class for any scorers that consume a Solr document and extract\n",
    "            a specific signal from a Solr document\n",
    "\n",
    "            Args:\n",
    "                name (str): Name of the Scorer\n",
    "                short_name (str): Used for the header which is sent to ranker\n",
    "                description (str): Description of the scorer\n",
    "        \"\"\"\n",
    "        super(UpVoteScorer, self).__init__(name=name, short_name=short_name, description=description)\n",
    "\n",
    "    def score(self, document):\n",
    "        upVote = document['upModVotes']\n",
    "\n",
    "        if upVote is not None:\n",
    "            if upVote == 0:\n",
    "                return 0\n",
    "            elif 0 < upVote <= 3:\n",
    "                return 0.15\n",
    "            elif 3 < upVote <= 5:\n",
    "                return 0.35\n",
    "            elif 5 < upVote <= 8:\n",
    "                return 0.55\n",
    "            elif 8 < upVote <= 11:\n",
    "                return 0.75\n",
    "            elif 11 < upVote <= 14:\n",
    "                return 0.85\n",
    "            elif upVote > 14:\n",
    "                return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "----------------------------------------------------------------------------\n",
    "\n",
    "Notice that the scorer uses the information stored in document (upModVotes). Make sure the data you need to use in the scorer is stored in your solr documents.\n",
    "\n",
    "Some preprocessing might be required in order to define the logic of the scorer (the normalization criteria, for example). For this example, a histogram of up votes was analyzed in order to define the normalization ranges.\n",
    "\n",
    "Once you have created the class that implements the custom socrer, edit the file features.json within the config directory of the project rr_custom_scorers_proxy_app to add the custom feature information. It should look like this:\n",
    "\n",
    "{\n",
    "  \"scorers\":[\n",
    "    {\n",
    "      \"init_args\":{\n",
    "        \"name\":\"UpVoteScorer\",\n",
    "        \"short_name\":\"uvs1\",\n",
    "        \"description\":\"Score based on the number of up votes a document received\"\n",
    "      },\n",
    "      \"type\":\"document\",\n",
    "      \"module\":\"document_upvote_scorer\",\n",
    "      \"class\":\"UpVoteScorer\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "If you write more than one scorer, just add them to the scorers:[] list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies and Compile Custom Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import shlex\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "\n",
    "CUSTOM_SCORERS_PATH=curdir+'/../custom-scorer'\n",
    "\n",
    "#creating wheel in /custom-scorer/ and copying it using pip install\n",
    "#Note: Wheel is a packaging framework for Python and is used\n",
    "#for packaging the custom scorer project\n",
    "\n",
    "try:\n",
    "    os.system(\"cd \"+CUSTOM_SCORERS_PATH+\"; pip wheel .\")\n",
    "    os.system(\"pip install retrieve_and_rank_scorer-0.0.1-py2-none-any.whl\")\n",
    "    print('Successfully installed retrieve_and_rank_scorer-0.0.1-py2-none-any.whl package.')\n",
    "except:\n",
    "    print ('Command to install custom scorer whl file failed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create service.cfg File\n",
    "\n",
    "This step writes your RR credentials to the service.cfg file, expected by the proxy app.\n",
    "To create the file, run the code below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "\n",
    "#loading credentials\n",
    "credFilePath = curdir+'/../config/credentials.json'\n",
    "with open(credFilePath) as credFile:\n",
    "    credentials = json.load(credFile)\n",
    "\n",
    "SERVICE_CFG_PATH=curdir+'/../config'\n",
    "\n",
    "#creating service.cfg file\n",
    "serviceCfgPath = SERVICE_CFG_PATH+'/service.cfg'\n",
    "with open(serviceCfgPath, 'w') as serviceCfgFile:\n",
    "    serviceCfgFile.write('SOLR_CLUSTER_ID='+credentials['cluster_id']+'\\n')\n",
    "    serviceCfgFile.write('SOLR_COLLECTION_NAME='+credentials['collection_name']+'\\n')\n",
    "    serviceCfgFile.write('RETRIEVE_AND_RANK_BASE_URL=https://gateway.watsonplatform.net/retrieve-and-rank/api'+'\\n')\n",
    "    serviceCfgFile.write('RETRIEVE_AND_RANK_USERNAME='+credentials['username']+'\\n')\n",
    "    serviceCfgFile.write('RETRIEVE_AND_RANK_PASSWORD='+credentials['password']+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start the Server\n",
    "\n",
    "Start the python flask server by using your command line or terminal as shown below.\n",
    "\n",
    "$ python server.py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Training Data\n",
    "\n",
    "Once the training ground truth file is ready, a file which contains the feature vectors for each questions needs to be generated. This file will also contain the new features added by your custom scorers.\n",
    "\n",
    "To generate the traingdata.csv file:\n",
    "\n",
    "0. Make sure the proxy app is running\n",
    "1. Edit bin/python/trainproxy.py - fl (fields) in lines 83, 95 to consider the fields used by the added custom scorers\n",
    "2. Run the code below (this is composed of two phase: generation of a trainingdata.csv file and sending the \n",
    "    request to create a ranker to the service). You know the request has been sent when the output shows something\n",
    "    like: {\"ranker_id\":\"3b140ax15-rank-2018\", \"name\":\"rr_ask_ranker_cs\", \"created\":\"2016-05-19T14:51:50.635Z\",   \n",
    "    \"url\":\"https://gateway.watsonplatform.net/retrieve-and-rank/api/v1/rankers/3b140ax15-\n",
    "    rank-2018\",\"status\":\"Training\",\"status_description\":\"The ranker instance is in its training phase\"}\n",
    "3. Validate trainingdata.csv has the new feature\n",
    "    (e.g. question_id,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,f10,f11,f12,r1,r2,s,newfeature,ground_truth)\n",
    "    \n",
    "### NOTE : THIS STEP COULD TAKE A LONG TIME! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import shlex\n",
    "import os\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "\n",
    "#loading credentials\n",
    "credFilePath = curdir+'/../config/credentials.json'\n",
    "with open(credFilePath) as credFile:\n",
    "    credentials = json.load(credFile)\n",
    "\n",
    "USERNAME=credentials['username']\n",
    "PASSWORD=credentials['password']\n",
    "SOLR_CLUSTER_ID=credentials['cluster_id']\n",
    "COLLECTION_NAME=credentials['collection_name']\n",
    "TRAIN_FILE_PATH=curdir+'/../bin/python'\n",
    "GROUND_TRUTH_FILE=curdir+\"/../data/groundtruth/answerGT_train.csv\"\n",
    "\n",
    "#Running command that trains a ranker\n",
    "cmd = 'python %s/trainproxy.py -u %s:%s -i %s -c %s -x %s -n %s' %\\\n",
    "    (TRAIN_FILE_PATH, USERNAME, PASSWORD, GROUND_TRUTH_FILE, SOLR_CLUSTER_ID, COLLECTION_NAME, \"ranker\")\n",
    "try:\n",
    "    process = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE)\n",
    "    output = process.communicate()[0]\n",
    "except:\n",
    "    print ('Command:')\n",
    "    print (cmd)\n",
    "    print ('Response:')\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train a New Ranker with Custom Scorer(s)\n",
    "\n",
    "To train a ranker which will consider the added features:\n",
    "\n",
    "0. Make sure the proxy app is running\n",
    "1. Run the code below\n",
    "\n",
    "    NOTE: In this step you will need to provide the value for the RANKER_NAME constant. \n",
    "\n",
    "### NOTE : THIS STEP COULD TAKE A LONG TIME! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import shlex\n",
    "import os\n",
    "\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "\n",
    "#loading credentials\n",
    "credFilePath = curdir+'/../config/credentials.json'\n",
    "with open(credFilePath) as credFile:\n",
    "    credentials = json.load(credFile)\n",
    "    \n",
    "\n",
    "BASEURL=credentials['url']\n",
    "RANKER_URL=BASEURL+\"rankers\"\n",
    "USERNAME=credentials['username']\n",
    "PASSWORD=credentials['password']\n",
    "TRAINING_DATA=curdir+'/../data/groundtruth/trainingdata.csv'\n",
    "\n",
    "#please provide the ranker name\n",
    "RANKER_NAME=\"rr_ask_cs_ranker\"\n",
    "\n",
    "#Checking if ranker with same name already exists\n",
    "curl_cmd = 'curl -u \"%s\":\"%s\" \"%s\"' %(USERNAME, PASSWORD, RANKER_URL)\n",
    "process = subprocess.Popen(shlex.split(curl_cmd), stdout=subprocess.PIPE)\n",
    "output = process.communicate()[0]\n",
    "found = False\n",
    "ranker_id = ''\n",
    "try:\n",
    "    parsed_json = json.loads(output)\n",
    "    rankers = parsed_json['rankers']\n",
    "    for i in range(len(rankers)):\n",
    "        ranker_json = rankers[i]\n",
    "        if ranker_json['name'] == RANKER_NAME:\n",
    "            found = True\n",
    "            ranker_id = ranker_json['ranker_id']\n",
    "except:\n",
    "    print ('Command:')\n",
    "    print (curl_cmd)\n",
    "    print ('Response:')\n",
    "    print (output) \n",
    "\n",
    "if found:\n",
    "    print \"Ranker \"+RANKER_NAME+\" already exists with ID \"+ranker_id+\".\"\n",
    "    print json.dumps(parsed_json, sort_keys=True, indent=4)\n",
    "else:\n",
    "    #Running command that trains a ranker\n",
    "    cmd = 'curl -k -X POST -u %s:%s -F training_data=@%s -F training_metadata=\"{\\\\\"name\\\\\":\\\\\"%s\\\\\"}\" %s' %\\\n",
    "        (USERNAME, PASSWORD, TRAINING_DATA, RANKER_NAME, RANKER_URL)\n",
    "    process = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE)\n",
    "    output = process.communicate()[0]\n",
    "    print cmd\n",
    "    try:\n",
    "        parsed_json = json.loads(output)\n",
    "        print json.dumps(parsed_json, sort_keys=True, indent=4)\n",
    "        credentials['cs_ranker_id'] = parsed_json['ranker_id']\n",
    "        with open(credFilePath, 'w') as credFileUpdated:\n",
    "            json.dump(credentials, credFileUpdated)\n",
    "            \n",
    "    except:\n",
    "        print ('Command:')\n",
    "        print (cmd)\n",
    "        print ('Response:')\n",
    "        print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check Status of a Ranker with Custom Scorers\n",
    "\n",
    "If the generation of training data finished succesfully and the output was that a new ranker has been created and it is on its training phase, you can query the status to check when it becomes \"Available\" by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import shlex\n",
    "import os\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "\n",
    "#loading credentials\n",
    "credFilePath = curdir+'/../config/credentials.json'\n",
    "with open(credFilePath) as credFile:\n",
    "    credentials = json.load(credFile)\n",
    "\n",
    "BASEURL=credentials['url']\n",
    "RANKER_URL=BASEURL+\"rankers\"\n",
    "USERNAME=credentials['username']\n",
    "PASSWORD=credentials['password']\n",
    "RANKER_ID=credentials['cs_ranker_id']\n",
    "\n",
    "#Running command that checks the status of a ranker\n",
    "curl_cmd = 'curl -u %s:%s %s/%s' % (USERNAME, PASSWORD, RANKER_URL, RANKER_ID)\n",
    "process = subprocess.Popen(shlex.split(curl_cmd), stdout=subprocess.PIPE)\n",
    "output = process.communicate()[0]\n",
    "try:\n",
    "    parsed_json = json.loads(output)\n",
    "    print json.dumps(parsed_json, sort_keys=True, indent=4)\n",
    "except:\n",
    "    print ('Command:')\n",
    "    print (curl_cmd)\n",
    "    print ('Response:')\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note : Before running experiments, you should update the .env file with the ranker id of the trained ranker and restart the plython flask application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Experiments with New Ranker using Custom Scorer(s)\n",
    "\n",
    "To test a ranker using custom scorers:\n",
    "\n",
    "0. Make sure the proxy app is running\n",
    "1. Edit bin/python/testproxy.py\n",
    "2. Edit fl (fields) in lines 85, 178 to consider the fields used by the added custom scorers\n",
    "3. Run the code below to:\n",
    "    - Edit service.cfg\n",
    "    - Create experiments folder in /data (if you run these multiple times, rename the folder to keep all of the\n",
    "        wanted results otherwise it will be overwritten)\n",
    "    - Run experiment_with_custom_scorers.sh to run the test experiment. This step will generate the files needed\n",
    "        for analysis of the custom scorers performance\n",
    "\n",
    "### NOTE : THIS STEP COULD TAKE A LONG TIME! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import shlex\n",
    "import os\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "\n",
    "#please provide the full path to the file answerGT_test.csv\n",
    "TEST_FILE=curdir+\"/../data/groundtruth/answerGT_test.csv\"\n",
    "\n",
    "#loading credentials\n",
    "credFilePath = curdir+'/../config/credentials.json'\n",
    "with open(credFilePath) as credFile:\n",
    "    credentials = json.load(credFile)\n",
    "\n",
    "#editing service.cfg file\n",
    "SERVICE_CFG_PATH=curdir+'/../config'\n",
    "\n",
    "SERVICE_CFG_PATH = SERVICE_CFG_PATH+'/service.cfg'\n",
    "\n",
    "#adding ranker and test file\n",
    "with open(SERVICE_CFG_PATH, 'w') as serviceCfgFile:\n",
    "    serviceCfgFile.write('SOLR_CLUSTER_ID='+credentials['cluster_id']+'\\n')\n",
    "    serviceCfgFile.write('SOLR_COLLECTION_NAME='+credentials['collection_name']+'\\n')\n",
    "    serviceCfgFile.write('RETRIEVE_AND_RANK_BASE_URL=https://gateway.watsonplatform.net/retrieve-and-rank/api'+'\\n')\n",
    "    serviceCfgFile.write('RETRIEVE_AND_RANK_USERNAME='+credentials['username']+'\\n')\n",
    "    serviceCfgFile.write('RETRIEVE_AND_RANK_PASSWORD='+credentials['password']+'\\n')\n",
    "    serviceCfgFile.write('RANKER_ID='+credentials['ranker_id']+'\\n')\n",
    "    serviceCfgFile.write('TEST_RELEVANCE_FILE='+TEST_FILE+'\\n')\n",
    "    \n",
    "#creating experiments directory\n",
    "curdir = os.getcwd()\n",
    "\n",
    "DATA_PATH=curdir+'/../data'\n",
    "\n",
    "cmd1 = 'mkdir '+DATA_PATH+'/experiments'\n",
    "try:\n",
    "    process = subprocess.Popen(shlex.split(cmd1), stdout=subprocess.PIPE)\n",
    "    output = process.communicate()[0]\n",
    "except:\n",
    "    print ('Command to create experiments directory failed:')\n",
    "    print (cmd1)\n",
    "\n",
    "#running experiment script\n",
    "try:\n",
    "    os.system(\"cd \"+curdir+\"/../; ./bin/bash/experiment.sh \"+SERVICE_CFG_PATH\\\n",
    "              +\" \"+DATA_PATH+\"/experiments\")\n",
    "except:\n",
    "    print ('Command to run experiment failed.')\n",
    "\n",
    "#changing ranker id to reflect custom scorer ranker\n",
    "with open(SERVICE_CFG_PATH, 'w') as serviceCfgFile:\n",
    "    serviceCfgFile.write('SOLR_CLUSTER_ID='+credentials['cluster_id']+'\\n')\n",
    "    serviceCfgFile.write('SOLR_COLLECTION_NAME='+credentials['collection_name']+'\\n')\n",
    "    serviceCfgFile.write('RETRIEVE_AND_RANK_BASE_URL=https://gateway.watsonplatform.net/retrieve-and-rank/api'+'\\n')\n",
    "    serviceCfgFile.write('RETRIEVE_AND_RANK_USERNAME='+credentials['username']+'\\n')\n",
    "    serviceCfgFile.write('RETRIEVE_AND_RANK_PASSWORD='+credentials['password']+'\\n')\n",
    "    serviceCfgFile.write('RANKER_ID='+credentials['cs_ranker_id']+'\\n')\n",
    "    serviceCfgFile.write('TEST_RELEVANCE_FILE='+TEST_FILE+'\\n')\n",
    "    \n",
    "#running experiment with custom scorers script\n",
    "try:\n",
    "    os.system(\"cd \"+curdir+\"/../; ./bin/bash/experiment_with_custom_scorers.sh \"+SERVICE_CFG_PATH\\\n",
    "              +\" \"+DATA_PATH+\"/experiments/\")\n",
    "except:\n",
    "    print ('Command to run experiment with custom scorers failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Ranker with Custom Scorer(s)\n",
    "\n",
    "To test the ranker, submit a query and observe the results returned by running the command below\n",
    "\n",
    "    NOTE: In this step you should provide the value for the QUESTION constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import shlex\n",
    "import os\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "\n",
    "#loading credentials\n",
    "credFilePath = curdir+'/../config/credentials.json'\n",
    "with open(credFilePath) as credFile:\n",
    "    credentials = json.load(credFile)\n",
    "\n",
    "USERNAME=credentials['username']\n",
    "PASSWORD=credentials['password']\n",
    "RANKER_ID=credentials['cs_ranker_id']\n",
    "\n",
    "#please provide the query to test\n",
    "QUESTION=\"what is the best city to visit in brazil\"\n",
    "\n",
    "#Running command that queries Solr\n",
    "QUESTION = QUESTION.replace(\" \",\"%20\")\n",
    "curl_cmd = 'curl GET \"http://localhost:3000/api/custom_ranker?&q=%s&wt=json&fl=id,title,subtitle,answer,\\\n",
    "answerScore,userReputation,views,upModVotes,downModVotes,userId,username,tags,userId,username,authorUsername,authorUserId&\\\n",
    "wt=json&ranker_id=%s&fq=\\'\\'\"' %(QUESTION, RANKER_ID)\n",
    "print curl_cmd\n",
    "process = subprocess.Popen(shlex.split(curl_cmd), stdout=subprocess.PIPE)\n",
    "output = process.communicate()[0]\n",
    "try:\n",
    "    parsed_json = json.loads(output)\n",
    "    print json.dumps(parsed_json, sort_keys=True, indent=4) \n",
    "except:\n",
    "    print ('Command:')\n",
    "    print (curl_cmd)\n",
    "    print ('Response:')\n",
    "    print (output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze Experiment Results\n",
    "\n",
    "Using the iPython notebook launched in Step 2 (testing step), repeat iPython notebook analysis from Step 2. You will need to add a new experiment variable to add the experiment with custom features included in the analysis charts or calculating NDCG scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import functools\n",
    "import requests\n",
    "import random\n",
    "import seaborn as sns\n",
    "import analysis_utils as au\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style('darkgrid')\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "base_directory=curdir+'/../data'\n",
    "experiments_directory = os.path.join(base_directory, 'experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Solr experiment\n",
    "solr_experiment_path = os.path.join(experiments_directory, 'exp_solr_only.json')\n",
    "solr_experiment = au.RetrieveAndRankExperiment(experiment_file_path=solr_experiment_path)\n",
    "solr_entries = solr_experiment.experiment_entries\n",
    "\n",
    "# RR experiment\n",
    "rr_experiment_path = os.path.join(experiments_directory, 'exp_retrieve_and_rank.json')\n",
    "rr_experiment = au.RetrieveAndRankExperiment(experiment_file_path=rr_experiment_path)\n",
    "rr_entries = rr_experiment.experiment_entries\n",
    "\n",
    "# RR experiment with custom scorers\n",
    "rr_experiment_path_scorer = os.path.join(experiments_directory, 'exp_retrieve_and_rank_scorers.json')\n",
    "rr_experiment_scorer = au.RetrieveAndRankExperiment(experiment_file_path=rr_experiment_path_scorer)\n",
    "rr_entries_scorer = rr_experiment_scorer.experiment_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_solr(query, fq=None, wt='json', fl='id,title,subtitle,answer,answerScore,upModVotes', num_rows=10):\n",
    "    \" Query standalone Solr \"\n",
    "    params = dict(q=query, wt=wt, fl=fl, rows=num_rows)\n",
    "    if fq is not None:\n",
    "        params['fq'] = fq\n",
    "    return solr_experiment.rr_service.select(params)\n",
    "\n",
    "def query_retrieve_and_rank(query, fq=None, wt='json', fl='id,title,subtitle,answer,answerScore,upModVotes', num_rows=10):\n",
    "    \" Query the retrieve and rank API \"\n",
    "    ranker_id = rr_experiment.ranker_id\n",
    "    params = dict(q=query, ranker_id=ranker_id, wt=wt, fl=fl, rows=num_rows)\n",
    "    if fq is not None:\n",
    "        params['fq'] = fq\n",
    "    return rr_experiment.rr_service.fcselect(params)\n",
    "\n",
    "def get_doc_by_id(doc_id, query=None):\n",
    "    \" Get the solr document, if we have the id\"\n",
    "    query = '*:*' if query is None else query\n",
    "    resp = query_solr(query='*:*', fq='id:%s' % doc_id, fl='id,title,subtitle,answer,answerScore,upModVotes')\n",
    "    if resp.ok:\n",
    "        docs = resp.json().get('response', {}).get('docs', [])\n",
    "        if len(docs) > 0 and docs[0]['id'] == doc_id:\n",
    "            return docs[0]\n",
    "        elif len(docs) == 0:\n",
    "            raise ValueError('No docs returned. Response json : %r' % resp.json())\n",
    "        else:\n",
    "            raise ValueError('ID of top document does not match. Response json : %r' % resp.json())\n",
    "    else:\n",
    "        raise resp.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Experiment Analysis\n",
    "\n",
    "### Total Relevance\n",
    "\n",
    "Total Relevance measures, for each query sent to the ranker, the % of answers in the top X documents that were relevant.\n",
    "\n",
    "For example, let query X have 8 relevant documents. The first 5 documents in the response from the ranker are relevant but the next 5 documents are all irrelevant. Total Relevance would be calculated as follows:\n",
    "\n",
    "    Total Relevance@001 = 1 relevant document in top 1 / 1 possible relevant document in top 1 = 1.00\n",
    "    ...\n",
    "    Total Relevance@005 = 5 relevant documents in top 5 / 5 possible relevant documents in top 5 = 1.00\n",
    "    ...\n",
    "    Total Relevance@008 = 5 relevant documents in top 8 / 8 possible relevant documents in top 8 = 0.625\n",
    "    ...\n",
    "    Total Relevance@010 = 5 relevant documents in top 10 / 8 possible relevant documents in top 10 = 0.625\n",
    "\n",
    "Thus total relevance measures the % of documents in the top X documents that are relevant, as compared to the maximum number of relevant documents that could be returned in the top X.\n",
    "\n",
    "This concludes the custom features section. The proxy app is probably still running on a session. To interrupt the process, just do Ctrl+C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the function\n",
    "plot_total_relevance_at_n = functools.partial(au.plot_relevance_results, func=au.total_relevance_at_n,\n",
    "                                              xlabel='Documents@00N Index', ylabel='Relevance %',\n",
    "                                              title='Avg. % of Documents in Top N that are Relevant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_total_relevance_at_n([solr_entries, rr_entries,rr_entries_scorer],\n",
    "                          legend=['Solr','RR','RR with Custom Scorer'],\n",
    "                          title='Total relevance with UpVote scorer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discounted Cumulative Gain is an Information Retrieval Metric that takes into account the position and relevance of documents at different positions. Normalized Discounted Cumulative Gain normalizes the metric based on what the optimal ranking of results would be.\n",
    "\n",
    "Notation:\n",
    "rel_i = relevance of ith document\n",
    "\n",
    "DCG@00N = rel_1 + sum(rel_i / log2(i + 1) for i in range(1,n))\n",
    "\n",
    "NDCG@00N = DCG@00N / IDCG@00N\n",
    "\n",
    "IDCG@00N = \"DCG of the optimal ordering of a result set\"\n",
    "\n",
    "For example, consider a query that has 3 relevant documents. 1 of these 3 documents has relevance 2 and the other 2 have relevance 1, (and naturally all other documents have relevance 0). Assume the relevance of the documents in the result set is as follows\n",
    "\n",
    "RS = [1, 0, 2, 1, 0, ...] (first retrieved document has relevance 1, second has relevance 0, etc.)\n",
    "\n",
    "The optimal result set would have most relevant documents first. Here is the ideal ordering for this problem:\n",
    "IRS = [2, 1, 1, 0, 0, ...]\n",
    "\n",
    "After doing the math, we get:\n",
    "DCG@005  = 2.43\n",
    "IDCG@005 = 3.13\n",
    "NDCG@005 = 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "absolute_strategy_ndcg = functools.partial(au.experiment_average_ndcg, method='absolute')\n",
    "relative_strategy_ndcg = functools.partial(au.experiment_average_ndcg, method='relative')\n",
    "plot_absolute_ndcg = functools.partial(au.plot_relevance_results, func=absolute_strategy_ndcg,\n",
    "                                      xlabel='Documents@00N Index', ylabel='NDCG',\n",
    "                                      title='Absolute NDCG@00N')\n",
    "plot_relative_ndcg = functools.partial(au.plot_relevance_results, func=relative_strategy_ndcg,\n",
    "                                        xlabel='Documents@00N Index', ylabel='NDCG',\n",
    "                                        title='Relative NDCG@00N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_absolute_ndcg([solr_entries, rr_entries,rr_entries_scorer],\n",
    "                    legend=['Solr', 'RR','RR with custom scorer'],\n",
    "                    title='Absolute NDCG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_relative_ndcg([solr_entries, rr_entries,rr_entries_scorer],\n",
    "                    legend=['Solr', 'RR'],\n",
    "                    title='Relative NDCG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup and Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "#getting current directory\n",
    "curdir = os.getcwd()\n",
    "\n",
    "CUSTOM_SCORERS_ANSWERS_PATH=curdir+'/../data/groundtruth/'\n",
    "\n",
    "#Remove temporary answers csv files generated by custom scorers\n",
    "cmd1 = 'rm '+CUSTOM_SCORERS_ANSWERS_PATH+'answer_*.csv'\n",
    "\n",
    "try:\n",
    "    os.system(cmd1)\n",
    "    print (cmd1)\n",
    "except:\n",
    "    print ('Removal of .csv file from answers directory failed:')\n",
    "    print (cmd1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
